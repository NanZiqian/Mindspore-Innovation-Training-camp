{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/obersturrrm/anaconda3/envs/mindspore/lib/python3.9/site-packages/mindnlp/utils/download.py:29: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import mindspore\n",
    "import argparse\n",
    "import numpy as np\n",
    "import logging\n",
    "import mindspore.dataset as ds\n",
    "import os\n",
    "\n",
    "import json\n",
    "\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from mindspore.nn import CrossEntropyLoss\n",
    "from mindspore import nn, ops\n",
    "from mindspore.train.serialization import save_checkpoint\n",
    "from mindspore.dataset import TextFileDataset\n",
    "\n",
    "from mindnlp.transforms import BertTokenizer\n",
    "from mindnlp.modules import Accumulator\n",
    "from mindnlp.models import GPT2Config, GPT2LMHeadModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 超参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 6\n",
    "batch_size = 8\n",
    "\n",
    "lr = 1e-4\n",
    "accumulate_step = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "自定义数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_path = './data/dialogues_train.json'\n",
    "test_path = './data/dialogues_test.json'\n",
    "eval_path = './data/dialogues_validation.json'\n",
    "train_dataset = TextFileDataset(str(train_path), shuffle=False)\n",
    "test_dataset = TextFileDataset(str(test_path), shuffle=False)\n",
    "eval_dataset = TextFileDataset(str(eval_path), shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 预处理\n",
    "article: [CLS] xxxxx [SEP]\n",
    "\n",
    "summary: [CLS] xxxxx [SEP]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# batch_size=8\n",
    "# [lcp]article[sep]summary[sep]的最大长度max_seq_len=1024\n",
    "def process_dataset(dataset, tokenizer, batch_size=8, max_seq_len=1024, shuffle=False):\n",
    "    def read_map(text):\n",
    "        data = json.loads(text.tobytes())\n",
    "        return np.array(data['article']), np.array(data['summarization'])\n",
    "\n",
    "    # 将summary与article融合，article在前\n",
    "    # [lcp]article[sep]summary[sep]\n",
    "    def merge_and_pad(article, summary):\n",
    "        article_len = len(article)\n",
    "        summary_len = len(summary)\n",
    "\n",
    "        sep_id = np.array([tokenizer.sep_token_id])\n",
    "        pad_id = np.array([tokenizer.pad_token_id])\n",
    "        # 若article+summary过长\n",
    "        if article_len + summary_len > max_seq_len:\n",
    "            # 缩短正文长度\n",
    "            new_article_len = max_seq_len - summary_len\n",
    "            merged = np.concatenate([article[:new_article_len], sep_id, summary[1:]])\n",
    "        elif article_len + summary_len - 1 < max_seq_len:\n",
    "            pad_len = max_seq_len - article_len - summary_len + 1\n",
    "            pad_text = np.array([tokenizer.pad_token_id] * pad_len)\n",
    "            merged = np.concatenate([article, summary[1:], pad_text])\n",
    "        else:\n",
    "            merged = np.concatenate([article, summary[1:]])\n",
    "            \n",
    "        return merged.astype(np.int32)\n",
    "\n",
    "    dataset = dataset.map(read_map, 'text', ['article', 'summary'])\n",
    "    dataset = dataset.map(tokenizer, 'article')\n",
    "    dataset = dataset.map(tokenizer, 'summary')\n",
    "    dataset = dataset.map(merge_and_pad, ['article', 'summary'], ['input_ids'])\n",
    "    \n",
    "    dataset = dataset.batch(batch_size)\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(batch_size)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# tokenize英文需要改为uncased, 中文为chinese\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': Tensor(shape=[8, 1024], dtype=Int32, value=\n",
      "[[ 101, 2360, 1010 ...    0,    0,    0],\n",
      " [ 101, 2017, 2113 ...    0,    0,    0],\n",
      " [ 101, 2054, 2079 ...    0,    0,    0],\n",
      " ...\n",
      " [ 101, 1045, 6592 ...    0,    0,    0],\n",
      " [ 101, 2008, 1005 ...    0,    0,    0],\n",
      " [ 101, 4165, 2307 ...    0,    0,    0]])}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "30522"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = process_dataset(train_dataset, tokenizer)\n",
    "eval_dataset = process_dataset(eval_dataset, tokenizer)\n",
    "test_dataset = process_dataset(test_dataset, tokenizer)\n",
    "# 查看第一个数据\n",
    "for data in train_dataset.create_dict_iterator():\n",
    "    print(data)\n",
    "    break \n",
    "\n",
    "# size of dictionary\n",
    "len(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练模型\n",
    "\n",
    "auto_mixed_precision\n",
    "\n",
    "混合精度预示着有不止一种精度的Tensor，那在PyTorch的AMP模块里是几种呢？2种：torch.FloatTensor和torch.HalfTensor；自动预示着Tensor的dtype类型会自动变化，也就是框架按需自动调整tensor的dtype（其实不是完全自动，有些地方还是需要手工干预）；"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from mindnlp._legacy.amp import auto_mixed_precision\n",
    "\n",
    "config = GPT2Config(vocab_size=len(tokenizer))\n",
    "model = GPT2LMHeadModel(config, ignore_index=tokenizer.pad_token_id)\n",
    "\n",
    "model = auto_mixed_precision(model, 'O1')\n",
    "\n",
    "optimizer = nn.AdamWeightDecay(model.trainable_params(), lr)\n",
    "# 梯度累加，将多次计算得到的梯度值进行累加，然后一次性进行参数更新\n",
    "accumulator = Accumulator(optimizer, accumulate_step, max_grad_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from mindspore import ops, ms_function\n",
    "from mindspore.amp import  all_finite, DynamicLossScaler\n",
    "from mindspore.amp import init_status\n",
    "\n",
    "# 动态调整损失缩放系数的管理器\n",
    "loss_scaler = DynamicLossScaler(scale_value=2**10, scale_factor=2, scale_window=1000)\n",
    "# Define forward function\n",
    "def forward_fn(input_ids, labels):\n",
    "    outputs = model(input_ids, labels=labels)\n",
    "    loss = outputs[0]\n",
    "    return loss_scaler.scale(loss / accumulate_step)\n",
    "\n",
    "# Get gradient function\n",
    "grad_fn = ops.value_and_grad(forward_fn, None, model.trainable_params())\n",
    "\n",
    "# Define function of one-step training\n",
    "@ms_function\n",
    "def train_step(data, label):\n",
    "    status = init_status()\n",
    "    data = ops.depend(data, status)\n",
    "    loss, grads = grad_fn(data, label)\n",
    "    loss = loss_scaler.unscale(loss)\n",
    "\n",
    "    is_finite = all_finite(grads, status)\n",
    "    if is_finite:\n",
    "        grads = loss_scaler.unscale(grads)\n",
    "        loss = ops.depend(loss, accumulator(grads))\n",
    "    loss = ops.depend(loss, loss_scaler.adjust(is_finite))\n",
    "    return loss, is_finite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "total = train_dataset.get_dataset_size()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    with tqdm(total=total) as progress:\n",
    "        progress.set_description(f'Epoch {epoch}')\n",
    "        loss_total = 0\n",
    "        cur_step_nums = 0\n",
    "        for batch_idx, (input_ids,) in enumerate(train_dataset.create_tuple_iterator()):\n",
    "            cur_step_nums += 1\n",
    "            loss, is_finite = train_step(input_ids, input_ids)\n",
    "            loss_total += loss\n",
    "\n",
    "            progress.set_postfix(loss=loss_total/cur_step_nums, finite=is_finite, scale_value=loss_scaler.scale_value.asnumpy())\n",
    "            progress.update(1)\n",
    "        save_checkpoint(model, f'gpt_summarization_epoch_{epoch}.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 验证\n",
    "\n",
    "加载系数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mindnlp._legacy.amp import auto_mixed_precision\n",
    "\n",
    "config_eval = GPT2Config(vocab_size=len(tokenizer))\n",
    "model_eval = GPT2LMHeadModel(config_eval, ignore_index=tokenizer.pad_token_id)\n",
    "model_eval = auto_mixed_precision(model_eval, 'O1')\n",
    "\n",
    "param_dict = load_checkpoint('gpt_epoch_0.ckpt')\n",
    "param_not_load, _ = load_param_into_net(model_eval, param_dict)\n",
    "print(param_not_load)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试精准度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mindnlp.engine import Evaluator\n",
    "from mindnlp.metrics import Accuracy\n",
    "\n",
    "metric = Accuracy()\n",
    "\n",
    "evaluator = Evaluator(network=model, eval_dataset=test_dataset, metrics=metric)\n",
    "evaluator.run(tgt_columns=\"summarization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 推理(聊天)\n",
    "\n",
    "加载系数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "from mindnlp._legacy.amp import auto_mixed_precision\n",
    "\n",
    "config_eval = GPT2Config(vocab_size=len(tokenizer))\n",
    "model_eval = GPT2LMHeadModel(config_eval, ignore_index=tokenizer.pad_token_id)\n",
    "model_eval = auto_mixed_precision(model_eval, 'O1')\n",
    "\n",
    "param_dict = mindspore.load_checkpoint('./gpt_epoch_0.ckpt')\n",
    "param_not_load, _ = mindspore.load_param_into_net(model_eval, param_dict)\n",
    "print(param_not_load)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "推理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i \n",
      "i ' \n",
      "i ' m \n",
      "i ' m afraid \n",
      "i ' m afraid it \n",
      "i ' m afraid it ' \n",
      "i ' m afraid it ' s \n",
      "i ' m afraid it ' s a \n",
      "i ' m afraid it ' s a little \n",
      "i ' m afraid it ' s a little bit \n",
      "i ' m afraid it ' s a little bit . \n",
      "i ' m afraid it ' s a little bit . [SEP] \n"
     ]
    }
   ],
   "source": [
    "article = 'This is Mr Meng speaking , Michelle .'\n",
    "\n",
    "def inference(article, tokenizer, model, max_seq_len):\n",
    "    input_ids = tokenizer.encode(article).ids\n",
    "    summary = ''\n",
    "    inputs = mindspore.Tensor(input_ids, mindspore.int64)\n",
    "    #inputs = ops.concat((inputs,mindspore.Tensor([101],mindspore.int64)))\n",
    "\n",
    "    for i in range(max_seq_len):\n",
    "        # model的输出: (logits, attn_scores, ...)\n",
    "        # logits:\n",
    "        # [batch_size, seq_len, vocab_size]\n",
    "        logits = model(inputs)[0]\n",
    "        \n",
    "        # pred\n",
    "        # [batch_size, seq_len]，取sep的单词值\n",
    "        pred = logits.argmax(-1)\n",
    "        pred = pred[-1].view(-1)\n",
    "        \n",
    "        # input : [cls] article [sep] summary1, summary2\n",
    "        inputs = ops.concat((inputs,pred))\n",
    "        summary += tokenizer.id_to_token(pred[0].asnumpy())\n",
    "        summary += ' '\n",
    "        print(summary)\n",
    "        if pred[0].asnumpy() == tokenizer.sep_token_id:\n",
    "            break\n",
    "\n",
    "    return summary\n",
    "\n",
    "summary = inference(article,tokenizer,model_eval,100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
